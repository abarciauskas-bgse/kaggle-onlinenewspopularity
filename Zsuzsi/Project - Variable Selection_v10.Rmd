---
title: "Stochastic Search Bayesian Variable Selection"
author: 'Group 2: Zsuzsa Holler, Nikitas Nikitas, Anneke Speijers'
date: "24 December 2015"
output: pdf_document
---

We start by simluating data for 50 variables. The first 40 variables have a true coefficient of zero, the next five have a true coefficient of 0.5 and the last five have a true coefficient of 1. The variables are highly correlated with one another ($\rho_{i,j} = 0.75$ for all variables). The data is generated by taking 100 random draws from a multivariate normal distribution.

We then run a variable selection algorithm for 1000 iterations and compare the results to the true data.   
   

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(mvtnorm)
library(mombf)
# true coefficients of variables 1-50: 
# First 40 variables have no influence, next 5 have a bit and last 5 have a lot.
w.true <- c(rep(0,40), rep(.5,5), rep(1,5))
m <- length(w.true)

# create covariance matrix - highly correlated variables. 
sigma <- diag(m)
sigma[upper.tri(sigma)] <- 0.75
sigma[lower.tri(sigma)] <- 0.75

# simulate data using above covariance and mean zero - n observations
n <- 100
set.seed(12345)
PHI <- rmvnorm(n, sigma=sigma)
PHI <- cbind(1, PHI) #add in intercept

# simulate outomes - standard normal error added to each one
y <- PHI[,-1] %*% matrix(w.true, ncol=1) + rnorm(n)
e = y - mean(y)

                       
### Gibbs Sampling
# number of iterations
L <- 1000

# Prior on model space using Beta-Binomial. Compute and save it outside the loop for every possible model size.
d.seq <- seq(0, m, 1)
model.prior <- rep(0, m+1)

for (i in 1:(m+1)) {
model.prior[i] <- 1/(m+1) * 1/( factorial(m) / (factorial(m - d.seq[i]) * factorial(d.seq[i])) )
}

# initial gamma
gamma.0 <- rep(0,50)
gamma <- gamma.0

# prior dispersion - we could extend this to run for various values of g
g <- n

#setup matrices to store results in each iteration
gamma.history <- matrix(NA, L, m)
prob.history <- rep(NA,L)
w.history <- matrix(NA, L, m)

# update loop
set.seed(14234)
for (i in 1:L) {
    for (j in 1:m) {
      
      #Compute probabilities without var j
      #number of parameters and gamma without j
      d.0 <- sum(gamma[-j]!=0) 
      gamma.test.0 <- gamma
      gamma.test.0[j] <- 0
      #Compute MLE r-squared without j
      PHI.0 <- PHI[,c(TRUE,(gamma.test.0==1))] 
      H.mle.0 <- PHI.0%*%solve(t(PHI.0)%*%PHI.0)%*%t(PHI.0)
      e.0 <- (diag(n)-H.mle.0) %*% y 
      R.sq.0 <- as.numeric( 1 - (t(e.0) %*% e.0) / sum((y - mean(y))^2) )
      #Compute second term of marginal likelihood (first term same for all models).
      marg.lik.0 <- as.numeric( (1+g)^((n-d.0-2)/2) / (1+g*(1-R.sq.0))^((n-1)/2))
      #Compute model probability with var j
      #number of parameters and gamma with j
      d.1 <- d.0 + 1
      gamma.test.1 <- gamma
      gamma.test.1[j] <- 1
      #Compute MLE r-squared with j
      PHI.1 <- PHI[,c(TRUE,(gamma.test.1==1))] 
      H.mle.1 <- PHI.1%*%solve(t(PHI.1)%*%PHI.1)%*%t(PHI.1)
      e.1 <- (diag(n)-H.mle.1) %*% y
      R.sq.1 <- as.numeric( 1 - (t(e.1) %*% e.1) / sum((y - mean(y))^2) )
      #Compute second term of marginal likelihood with j
      marg.lik.1 <- as.numeric( (1+g)^((n-(d.1)-2)/2) / (1+g*(1-R.sq.1))^((n-1)/2) )
        
      # compute probability of new variable being in model 
      prob <- marg.lik.1 * model.prior[d.1+1] / (marg.lik.1 * model.prior[d.1+1] + marg.lik.0 * model.prior[d.0+1]) 
        
      # update gamma and variables
      if (runif(1) < prob) {
        gamma[j] <- 1  
        }
      else {
        gamma[j] <- 0
        }
      
    }
    #Compute results for the new model
    d.new <- sum(gamma!=0) 
    PHI.new <- PHI[,c(TRUE,(gamma==1))] 
    H.mle.new <- PHI.new%*%solve(t(PHI.new)%*%PHI.new)%*%t(PHI.new)
    e.new <- (diag(n)-H.mle.new) %*% y 
    R.sq.new <- as.numeric( 1 - (t(e.new) %*% e.new) / sum((y - mean(y))^2) )
    marg.lik.new <- as.numeric(factorial((n-1)/2 - 1) / (pi^((n-1)/2) * sqrt(n)) * (sqrt(t(e) %*% e))^(1-n) * (1+g)^((n-d.new-2)/2) / (1+g*(1-R.sq.new))^((n-1)/2) )
    #Save w estimates (w computed based on the w_bayes estimate expression for Zellner's g prior)
    w.history[i,which(gamma!=0)] = ((g/ (g+1)) * solve(t(PHI.new)%*% PHI.new)%*%t(PHI.new)%*%y)[-1]
    #Save probabilities
    prob.new <- marg.lik.new * model.prior[d.new+1]
    prob.history[i] <- marg.lik.new
    #Save gamma
    gamma.history[i,] <- gamma
}
```


```{r, echo=FALSE}
#PLOTS

#Number of variables in model
no.vars <- rowSums(gamma.history)
plot(1:L, no.vars, type="l",xlab="Iteration", ylab="Number of variables in the model", main="Model size")
abline(mean(no.vars), 0, col="red")
```


Figure 1. The model size selected at each step of the iteration. We can see that the model size varies between 5 and 14 variables. The red line corresponds to the mean model size over all iterations (8.55).   

Below the top 5 most probable models are reported with the corresponding estimated posterior model probabilities. It can be seen that the first model has relatively high probability and all the other models have much smaller posterior probability.  

```{r, echo=FALSE}   
library(plyr)
model.prob = data.frame( model = apply( gamma.history,1, function(x) {paste(c(1:50)[as.logical(x)], collapse = ", ")}  ))

model.prob = ddply(model.prob, .(model), nrow)
model.prob$prob = model.prob$V1/L
model.prob = model.prob[order(model.prob$V1, decreasing =T),]

model.prob[1:5,c(1,3)]

```


```{r, echo=FALSE}
#Marginal inclusion probabilities of features
cummean <- matrix(NA, L, m)
for (i in 1:L) {
    cummean[i,] <- colSums(gamma.history[1:i,,drop=F])/i
}
matplot(1:L, cummean, type="l", lty=1, xlab="Step", ylab="marginal inclusion probability", main="Marginal Inclusion Probabilities for each of the 50 Features", las=1)
```

Figure 2. The above graph displays the marginal inclusion probabilities for each of the features. They appear to stabilise after ~300 iterations. There are 7 variables with a marginal inclusion probability > 0.6 and 2 with a marginal inclusion probability of approximately 0.2. The remaining features have a marginal inclusion probability of approximately 0. Which features these correspond to can be seen more clearly in the next graph.   
   

```{r, echo=FALSE}
#Final marginal inclusion probabilities
plot(cummean[1000,1:40], xlim=c(0,50), ylim=c(0,1), xlab="Predictor", ylab="Posterior probability of non-zero coefficient", main="Marginal Inclusion Probabilities")
points(41:45, cummean[1000, 41:45], col="blue")
points(46:50, cummean[1000, 46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
```

Figure 3. Black dots correspond to the first 40 features, blue dots to features 41 to 45, red dots to features 46 to 50. The variable selection algorithm is picking up all 5 of the features that have coefficient 1 in the real data quite well. For the 5 features that have a 0.5 coefficient in the real data, the algorithm doesn't do quite so well. Only 2 of them have marginal inclusion probability higher than 0.5. The algorithm picks up one variable with a probability of inclusion of ~0.2, that actually has no influence in the real data.   
   

```{r, echo=FALSE}
#Coefficient estimates
w.history[is.na(w.history)] <- 0
plot(colMeans(w.history, na.rm = TRUE)[1:40], xlim=c(0,50), ylim=c(-0.5, 1.5), xlab="Predictor", ylab="Expected posterior coefficients", main="Bayesian Model Averaging - Estimates of Coefficients")
points(41:45, colMeans(w.history, na.rm = TRUE)[41:45], col="blue")
points(46:50, colMeans(w.history, na.rm = TRUE)[46:50], col="red")
abline(1, 0, col="red")
abline(0.5, 0, col="blue")
```

Figure 4. The BMA estimates for the coefficients are displayed in the graph above. For the high importance variables, it estimates 3 of them quite well but overestimates the coefficients of two of them. For the medium importance variables, it slightly overestimates the coefficeints for two of them and completely misses the other 3. For the zero influence variables, BMA does quite well; it only slightly overestimates the coefficient for 1 of them.
