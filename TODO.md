# TODO

SUMMARIZE WHAT'S BEEN DONE SO FAR AND RESULTS

## Classification

* xgboost (r package)
* take [weighted] average of classifiers
* random forest
* testing using rolling windows of test sets / random / mcmc test sets
* knn with different distance metric
* we've done this with some sucess: classifying just 1,2,3
    * try other ways to group
* cross-validation to find optimal gamma in boosting
* n-fold cross validation to find (and ignore) outliers
* [aimee would like to do this for fun] implement adaboost
* libraray for adaboost

## Variable selection

* [BayesVarSel R Package Docs](https://cran.r-project.org/web/packages/BayesVarSel/BayesVarSel.pdf)
* bayes variable selection using bayes factors (base)
    * [reference](https://projecteuclid.org/download/pdf_1/euclid.ba/1340370391)
* gibbs sampling for variable selection
    * [reference](http://www.cs.berkeley.edu/~russell/classes/cs294/f05/papers/george+mcculloch-1993.pdf)
    * [reference](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.4258&rep=rep1&type=pdf)
* lasso

## Other

## Done

* simple k-nn
* simple logistic
* random forest
* gbm
* svm
* some bad feature selection (aimee: or at least my efforts have been hapless and unsuccessful)
* multinom (nnet)
* maybe more

