# TODO

## Classification

* xgboost (r package)
* take [weighted?] average of classifiers
* decision tree
* random forest

## Feature selection

* gibbs sampling for variable selection [reference](http://www.cs.berkeley.edu/~russell/classes/cs294/f05/papers/george+mcculloch-1993.pdf)

## Other

* compare results with python results
* train just on classes 1-3

## Done

* simple k-nn
* simple logistic
