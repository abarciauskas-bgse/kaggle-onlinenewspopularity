# TODO

SUMMARIZE WHAT'S BEEN DONE SO FAR AND RESULTS

## Classification

* xgboost (r package)
* take [weighted?] average of classifiers
* decision tree
* random forest
* testing using rolling windows of test sets
* knn with different distance metric
* groups of classes (e.g. 2's v 3's)
* cross-validation to find optimal gamma in boosting
* n-fold cross validation to find (and ignore) outliers
* adaboost

## Variable selection

* [BayesVarSel R Package Docs](https://cran.r-project.org/web/packages/BayesVarSel/BayesVarSel.pdf)
* bayes variable selection using bayes factors (base)
    * [reference](https://projecteuclid.org/download/pdf_1/euclid.ba/1340370391)
* gibbs sampling for variable selection
    * [reference](http://www.cs.berkeley.edu/~russell/classes/cs294/f05/papers/george+mcculloch-1993.pdf)
    * [reference](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.4258&rep=rep1&type=pdf)
* lasso

## Other

* compare results with python results
* train just on classes 1-3

## Done

* simple k-nn
* simple logistic
